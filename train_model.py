import pandas as pd
from feature_extractor import extract_feature
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.svm import SVC
import joblib
import numpy as np
import os
from collections import Counter
from sklearn.pipeline import make_pipeline

# ==========================
# 1. ãƒ‘ã‚¹è¨­å®š
# ==========================
DATASET_DIR = "dataset"
WAV_DIR = os.path.join(DATASET_DIR, "wav")
EVAL_DIR = os.path.join(DATASET_DIR, "eval")
CATEGORY_FILE = os.path.join(EVAL_DIR, "category.txt")

# ==========================
# 2. category.txt ã®èª­ã¿è¾¼ã¿
# ==========================
df = pd.read_csv(CATEGORY_FILE)
df = df.dropna(subset=['fid', 'ans1'])  # æ¬ æé™¤å»

# fid ã”ã¨ã«ä»£è¡¨ãƒ©ãƒ™ãƒ«ã‚’ä½¿ç”¨
fid_to_label = dict(zip(df['fid'], df['ans1']))

# ==========================
# 3. ç‰¹å¾´é‡æŠ½å‡º
# ==========================
X, y = [], []

for fid, label in fid_to_label.items():
    wav_path = os.path.join(WAV_DIR, f"{fid}.wav")

    if os.path.exists(wav_path):
        try:
            feat = extract_feature(wav_path)
            X.append(feat)
            y.append(label)
        except Exception as e:
            print(f"âŒ Error with {fid}: {e}")
    else:
        print(f"âš ï¸ Missing file: {wav_path}")

# numpyå¤‰æ›
X = np.array(X)
y = np.array(y)

# ==========================
# 4. ã‚¯ãƒ©ã‚¹æ•°ãŒ1ã®ã‚¯ãƒ©ã‚¹ã‚’é™¤å¤–ï¼ˆé‡è¦ï¼‰
#    äº¤å·®æ¤œè¨¼ã®ãŸã‚ã«å„ã‚¯ãƒ©ã‚¹ã«æœ€ä½2ã‚µãƒ³ãƒ—ãƒ«å¿…è¦
# ==========================
print("Label count BEFORE:", Counter(y))

# æœ€ä½ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’ 2 ã«è¨­å®šï¼ˆ1 ã‚µãƒ³ãƒ—ãƒ«ã®ã‚¯ãƒ©ã‚¹ã¯å‰Šé™¤ï¼‰
min_required_per_class = 2
valid_classes = {lab for lab, cnt in Counter(y).items() if cnt >= min_required_per_class}

X = np.array([x for x, lab in zip(X, y) if lab in valid_classes])
y = np.array([lab for lab in y if lab in valid_classes])

print("Label count AFTER:", Counter(y))

# åŸºæœ¬ãƒã‚§ãƒƒã‚¯ï¼šã‚¯ãƒ©ã‚¹æ•°ãŒ2æœªæº€ã®å ´åˆã¯å­¦ç¿’ä¸å¯
unique_labels = np.unique(y)
n_classes = len(unique_labels)
n_samples = len(y)

if n_classes < 2:
    raise ValueError(f"è¨“ç·´ã§ãã‚‹ã‚¯ãƒ©ã‚¹ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹æ•°={n_classes}ã€ã‚µãƒ³ãƒ—ãƒ«æ•°={n_samples}")

# ==========================
# 5. å±¤åŒ–äº¤å·®æ¤œè¨¼ï¼ˆè©•ä¾¡ï¼‰
#    ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„ã®ã§ StratifiedKFold ã‚’ä½¿ã£ã¦å®‰å®šè©•ä¾¡ã™ã‚‹
# ==========================
# å„ã‚¯ãƒ©ã‚¹ã®æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’æ±‚ã‚ã€ãã‚Œã«åˆã‚ã›ã¦ n_splits ã‚’æ±ºå®š
class_counts = Counter(y)
min_class_count = min(class_counts.values())

# n_splits ã¯ 2..5 ã®é–“ã§ã€min_class_count ã‚’è¶…ãˆãªã„å€¤ã«ã™ã‚‹
max_splits = 5
n_splits = min(max_splits, min_class_count)
if n_splits < 2:
    n_splits = 2  # å®‰å…¨æªç½®ï¼ˆãŸã ã— min_class_count ãŒ 1 ã®å ´åˆã¯ã“ã“ã«æ¥ã‚‹ã¹ãã§ãªã„ï¼‰

print(f"n_samples={n_samples}, n_classes={n_classes}, min_class_count={min_class_count}, using n_splits={n_splits} for StratifiedKFold")

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼šã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ + SVM
pipeline = make_pipeline(
    StandardScaler(),
    SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=42)
)

cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
scores = cross_val_score(pipeline, X, y, cv=cv, scoring='accuracy', n_jobs=-1)

print(f"âœ… Cross-validation accuracy scores: {scores}")
print(f"âœ… CV mean accuracy: {scores.mean():.4f} Â± {scores.std():.4f}")

# ==========================
# 6. æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’å…¨ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã—ã¦ä¿å­˜
#    å®Ÿè¡Œæ™‚ã«ã¯å…¨ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã—ã¦ãƒ‡ãƒ—ãƒ­ã‚¤ç”¨ãƒ¢ãƒ‡ãƒ«ã‚’ä½œã‚‹ã®ãŒä¸€èˆ¬çš„
# ==========================
# scaler ã‚’å€‹åˆ¥ã«ä¿å­˜ã—ãŸã‹ã£ãŸã®ã§ã€pipeline ã§ã¯ãªãå€‹åˆ¥ã« fit ã—ã¦ä¿å­˜ã™ã‚‹
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

clf = SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=42)
clf.fit(X_scaled, y)

acc = clf.score(X_scaled, y)
print(f"âœ… Training accuracy on full dataset: {acc:.4f}")

# ==========================
# 7. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
# ==========================
os.makedirs("model", exist_ok=True)
joblib.dump(clf, "model/classifier.pkl")
joblib.dump(scaler, "model/scaler.pkl")
np.save("model/labels.npy", np.unique(y))

print("ğŸ‰ Training complete! Model and scaler saved in ./model/")
