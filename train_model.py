import argparse
import pandas as pd
from feature_extractor import extract_feature
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.svm import SVC
import joblib
import numpy as np
import os
from collections import Counter
from sklearn.pipeline import make_pipeline
import sys

# ==========================
# CLI: --force ã§å†å­¦ç¿’ã‚’å¼·åˆ¶
# ==========================
parser = argparse.ArgumentParser(description="Train emotion recognition model (skip if already trained).")
parser.add_argument(
    "--force",
    action="store_true",
    help="Force retraining even if model files already exist.",
)
args = parser.parse_args()

# ==========================
# 1. ãƒ‘ã‚¹è¨­å®š
# ==========================
DATASET_DIR = "dataset"
WAV_DIR = os.path.join(DATASET_DIR, "wav")
EVAL_DIR = os.path.join(DATASET_DIR, "eval")
CATEGORY_FILE = os.path.join(EVAL_DIR, "category.txt")

MODEL_DIR = "model"
CLASSIFIER_FILE = os.path.join(MODEL_DIR, "classifier.pkl")
SCALER_FILE = os.path.join(MODEL_DIR, "scaler.pkl")
LABELS_FILE = os.path.join(MODEL_DIR, "labels.npy")

# ==========================
# 2. æ—¢ã«å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Œã°ä½•ã‚‚ã—ãªã„ï¼ˆ--force ã§ç„¡è¦–ï¼‰
# ==========================
if not args.force and os.path.exists(CLASSIFIER_FILE) and os.path.exists(SCALER_FILE) and os.path.exists(LABELS_FILE):
    print("â„¹ï¸  ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚å†å­¦ç¿’ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
    print(f"    {CLASSIFIER_FILE}")
    print(f"    {SCALER_FILE}")
    print(f"    {LABELS_FILE}")
    print("å¿…è¦ãªã‚‰ --force ã‚’ä»˜ã‘ã¦å†å­¦ç¿’ã—ã¦ãã ã•ã„ã€‚")
    sys.exit(0)

# ==========================
# 3. category.txt ã®èª­ã¿è¾¼ã¿
# ==========================
df = pd.read_csv(CATEGORY_FILE)
df = df.dropna(subset=['fid', 'ans1'])  # æ¬ æé™¤å»

# fid ã”ã¨ã«ä»£è¡¨ãƒ©ãƒ™ãƒ«ã‚’ä½¿ç”¨
fid_to_label = dict(zip(df['fid'], df['ans1']))

# ==========================
# 4. ç‰¹å¾´é‡æŠ½å‡º
# ==========================
X, y = [], []

for fid, label in fid_to_label.items():
    wav_path = os.path.join(WAV_DIR, f"{fid}.wav")

    if os.path.exists(wav_path):
        try:
            feat = extract_feature(wav_path)
            X.append(feat)
            y.append(label)
        except Exception as e:
            print(f"âŒ Error with {fid}: {e}")
    else:
        print(f"âš ï¸ Missing file: {wav_path}")

# numpyå¤‰æ›
X = np.array(X)
y = np.array(y)

# ==========================
# 5. ã‚¯ãƒ©ã‚¹æ•°ãŒ1ã®ã‚¯ãƒ©ã‚¹ã‚’é™¤å¤–ï¼ˆé‡è¦ï¼‰
#    äº¤å·®æ¤œè¨¼ã®ãŸã‚ã«å„ã‚¯ãƒ©ã‚¹ã«æœ€ä½2ã‚µãƒ³ãƒ—ãƒ«å¿…è¦
# ==========================
print("Label count BEFORE:", Counter(y))

# æœ€ä½ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’ 2 ã«è¨­å®šï¼ˆ1 ã‚µãƒ³ãƒ—ãƒ«ã®ã‚¯ãƒ©ã‚¹ã¯å‰Šé™¤ï¼‰
min_required_per_class = 2
valid_classes = {lab for lab, cnt in Counter(y).items() if cnt >= min_required_per_class}

X = np.array([x for x, lab in zip(X, y) if lab in valid_classes])
y = np.array([lab for lab in y if lab in valid_classes])

print("Label count AFTER:", Counter(y))

# åŸºæœ¬ãƒã‚§ãƒƒã‚¯ï¼šã‚¯ãƒ©ã‚¹æ•°ãŒ2æœªæº€ã®å ´åˆã¯å­¦ç¿’ä¸å¯
unique_labels = np.unique(y)
n_classes = len(unique_labels)
n_samples = len(y)

if n_classes < 2:
    raise ValueError(f"è¨“ç·´ã§ãã‚‹ã‚¯ãƒ©ã‚¹ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹æ•°={n_classes}ã€ã‚µãƒ³ãƒ—ãƒ«æ•°={n_samples}")

# ==========================
# 6. å±¤åŒ–äº¤å·®æ¤œè¨¼ï¼ˆè©•ä¾¡ï¼‰
#    ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„ã®ã§ StratifiedKFold ã‚’ä½¿ã£ã¦å®‰å®šè©•ä¾¡ã™ã‚‹
# ==========================
# å„ã‚¯ãƒ©ã‚¹ã®æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’æ±‚ã‚ã€ãã‚Œã«åˆã‚ã›ã¦ n_splits ã‚’æ±ºå®š
class_counts = Counter(y)
min_class_count = min(class_counts.values())

# n_splits ã¯ 2..5 ã®é–“ã§ã€min_class_count ã‚’è¶…ãˆãªã„å€¤ã«ã™ã‚‹
max_splits = 5
n_splits = min(max_splits, min_class_count)
if n_splits < 2:
    n_splits = 2  # å®‰å…¨æªç½®ï¼ˆãŸã ã— min_class_count ãŒ 1 ã®å ´åˆã¯ã“ã“ã«æ¥ã‚‹ã¹ãã§ãªã„ï¼‰

print(f"n_samples={n_samples}, n_classes={n_classes}, min_class_count={min_class_count}, using n_splits={n_splits} for StratifiedKFold")

# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼šã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ + SVM
pipeline = make_pipeline(
    StandardScaler(),
    SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=42)
)

cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
scores = cross_val_score(pipeline, X, y, cv=cv, scoring='accuracy', n_jobs=-1)

print(f"âœ… Cross-validation accuracy scores: {scores}")
print(f"âœ… CV mean accuracy: {scores.mean():.4f} Â± {scores.std():.4f}")

# ==========================
# 7. æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’å…¨ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã—ã¦ä¿å­˜
# ==========================
# scaler ã‚’å€‹åˆ¥ã«ä¿å­˜ã—ãŸã‹ã£ãŸã®ã§ã€pipeline ã§ã¯ãªãå€‹åˆ¥ã« fit ã—ã¦ä¿å­˜ã™ã‚‹
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

clf = SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=42)
clf.fit(X_scaled, y)

acc = clf.score(X_scaled, y)
print(f"âœ… Training accuracy on full dataset: {acc:.4f}")

# ==========================
# 8. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
# ==========================
os.makedirs(MODEL_DIR, exist_ok=True)
joblib.dump(clf, CLASSIFIER_FILE)
joblib.dump(scaler, SCALER_FILE)
np.save(LABELS_FILE, np.unique(y))

print(f"ğŸ‰ Training complete! Model and scaler saved in ./{MODEL_DIR}/")
